import itertools
import os
import sys
import dill as pickle
import csv
import json
import re
import string
import nltk
import spacy
import preprocessor as p
import emoji
from collections import Counter,defaultdict
from spacy_langdetect import LanguageDetector
from ftfy import fix_text
from ekphrasis.classes.preprocessor import TextPreProcessor
from ekphrasis.classes.tokenizer import SocialTokenizer
from ekphrasis.dicts.emoticons import emoticons
from ekphrasis.dicts.noslang.slangdict import slangdict
from twitter_preprocessor import TwitterPreprocessor
p.set_options(p.OPT.EMOJI,p.OPT.RESERVED,p.OPT.SMILEY)
nlp = spacy.load('en')
nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)
nlp1 = spacy.load('en_core_web_sm')
MIN_TOKENS=10
MAX_TOKENS=70
MAX_SEQ = 500000
all_emoticons = ['(ï¼ _ï¼ ;)', '~:>', '=p', ':â€‘.', '|-)', ':-]', '=P~', 'b-(', 'ğŸ˜œ', '(^â—‹^)', '(v)oï¿¥o(v)', 'o:-)', 'ğŸ™†', '(^_-)-â˜†', ':-\\', '(ï¼´â–½ï¼´)', '(*_*)', '(w)', ':o(', '<`ãƒ˜Â´>', 'à² _à² ', ';(', '(^ãƒ»^)', '(âœ¿â— â€¿â— )', ':)', '3:)', 'ğŸ˜—', 'ğŸ˜', '(/â—•ãƒ®â—•)/', 'ğŸ˜”', ':^)', 'x-p', '=-3', 'ğŸ™Š', '_(._.)_', '(^.^)/~~~', ":'-)", '8-0', ':((', 'ğŸ‚', '=D>', '>:p', ':-"', '(?_?)', 'ğŸ¤“', '(so)', 'o_O', ':)>-', ':â€‘j', 'ğŸ˜½', 't.t', '\\U0001f604', '^_^', ';â€‘]', 'ğŸ‘¶', 'ğŸ™', ',:-)', ':-L', ':-p', ':!!', '(^o^)ï¼', '[-(', 'ğŸ˜±', '(ï¼â€¸áƒš)', 'd=', '(+_+)', 'x-d', '=p~', '#:-S', ':â€‘|', ':â€‘Ã¾', 'ğŸ˜‡', ';^)', '=-D', 'ğŸ˜£', ':o3', '(~o~)', '(C)', 'ğŸ˜¿', "+/'\\", ':-j', 'ï¼ˆÂ´âˆ€ï½€ï¼‰', '%-(', '8-d', '0:â€‘3', '(Â´ï½¥Ï‰ï½¥`)', '(-""-)', '$-)', 'ï¼¼(â—oâ—)ï¼ï¼', 'ğŸ˜¥', ';d', ':-Ã¾', '<3', '<^!^>', '(;', '(Z)', 'ğŸ™…', ':###..', 'Q_Q', 'ğŸ˜’', '(._.)', '=:)', 'XP', '(6)', '=:o]', '(ï¼¾â–½ï¼¾)', '=((', '(:_;)', ':â€‘)', '<(ï½€^Â´)>', ':o)', '(I)', 'ğŸ‘½', 'ğŸ§', 'x-@', '(B)', '(^<^)', '(^^)', ':â€‘/', '@>â€‘â€‘>â€‘â€‘', "D-':", 'o/\\o', '=))', 'ğŸ˜™', '(E)', '\\U0001f44b', 'ğŸµ', 'â˜„ï¸', ':\\', '>:s', 'ğŸ˜„', ':-ss', '(>_<)>', ':-??', ';))', '(y)', ';-;', 'O:â€‘)', 'ğŸ˜‰', '(N)', 'ï¼ˆâ€™-â€™*)', 'ğŸ˜«', ':]', '@}->--', ':ã£c', '<):)', 'ğŸ˜®', '^o)', '(*^^)v', ':-#', ':^*', '>.<', '}:â€‘)', '(p)', 'Xâ€‘P', ':-(', '(ã‚œ_ã‚œ>)', ':-[', '8)', '(d)', '(o)', ':â€‘O', '(ãƒ»ãƒ»?', 'Â°OÂ°', ':p', 'ğŸ˜Ÿ', 'ğŸ˜¹', '(^)o(^)', '%%-', 'V.v.V', 'xâ€‘p', '(-_-ãƒ¡)', "d-':", '(ä¸€ä¸€)', '(li)', ':-P', ':o', '(__)', '<`ï½Â´>', '|-O', '(*^ã€‚^*)', 'ãƒ½(^o^)ä¸¿', 'ğŸ˜¤', '\\u0001f3fb', '(ToT)', 'o_o', ':X', ':|', '(8)', '(^ãƒ ^)', ':-*', '=/', 'DX', '(-_-)zzz', '~X(', '\\U0001f1f3', 'XD', '8-X', '#-o', "(';')", '(=^ãƒ»ãƒ»^=)', '(ãƒ¼ãƒ¼ã‚›)', ';n;', ':}', 'ğŸ˜€', 'O_o', '(&)', ':P', 'ğŸ˜“', 'ğŸ™Œ', ':â€‘J', ':ã£)', 'ğŸ™‰', '3:-o', '[-o<', '(^_^ãƒ¡)', ':â€‘X', 'xp', '\\U0001f64f', 'ğŸ¤”', '(sn)', '\\u0001f620', 'ğŸ™€', '8â€‘d', 'd:', '(^_^)/~', '+o(', '|â€‘o', ':-))', 'âˆ©(ãƒ»Ï‰ãƒ»)âˆ©', ':-&', ':â€‘o', '(||)', ':{', '(*Â°âˆ€Â°)=3', '>_>^', '\\:D/', '(~_~;)', '\\u0001f602', '8o|', '(})', ':<', 'ğŸ', '(ãƒ¼_ãƒ¼)!!', '>:-)', '\\u0001f64f', 'ğŸ’¯', 'ğŸ‘‹', 'l-)', '(~_~ãƒ¡)', '\\u0001f3c2', '(um)', "@}â€‘;â€‘'â€‘â€‘â€‘", '(:', 'Â°oÂ°', 'm(__)m', 'â˜ºï¸', '(-.-)', 'ğŸ˜³', ':â€‘#', '(-_-)', ':))', '8â€‘D', '~:â€‘\\', 'ğŸ˜–', '><>', '3:â€‘)', '>:D<', '(^J^)', 'ğŸ”˜', '</3', ':-x', '(^o^)ä¸¿', 'ï¼¼(-o-)ï¼', '(ã‚œ.ã‚œ)', '(^0_0^)', '><(((*>', ':â€‘[', 'D8', '>:S', '~o)', '(co)', 'ğŸ˜¡', '//0-0\\\\', "('_')", '(mp)', '<:-|', '(=_=)', 'x_x', '}:-)', '(g)', '//0â€‘0\\\\', ';-D', '(^_^.)', '=3', ':-.', '(^^;)', '(â€˜a`)', ':-@', 'ğŸ˜²', 'q.q', ')^o^(', 'ğŸ˜¦', '\\U0001f3c2', '8-)', 'ğŸ ', '(;_;)/~~~', 'ğŸ˜¯', 'ğŸ¦‡', '(;_:)', '8-D', '^#(^', '(t_t)/~~~', '(;O;)', '-D', ':O', 'ğŸ˜', ':@', ':-0', '<(o0o)>', '((d[-_-]b))', '(%)', '*-)', '>:[', '(^.^)', ':-###..', 'ğŸ˜ª', '0:â€‘)', ':&', 'â˜º', '(ãƒ»_ãƒ»;)', '(tot)', '(ip)', ':(', 'ğŸ˜ƒ', '(au)', 'o_0', '\\u0001f61c', '\\U0001f602', 'B^D', ':-Ã', '@};-', ':^(', 'ğŸ˜ ', '(^_^)v', ':d', 'ï¼ˆâŒ’â–½âŒ’ï¼‰', '(ã‚œãƒ¬ã‚œ)', ':â€‘P', '(ToT)/~~~', ':*', 'ğŸ˜', ':bz', '(bah)', 'o.O', ';-d', '%)', ":'-(", ':ã£C', 'ğŸ˜ˆ', 'o.o', '(>_<)', '(K)', 'ğŸ‡³', 'ğŸ¤‘', 'ğŸ±', '~x(', '(b)', 'ğŸ¤¦', '(ãƒ»ã€‚ãƒ»;)', '=]', '#-)', 'O:)', '({)', ':)]', 'ï¼ˆãƒ»âˆ€ãƒ»ï¼‰', 'ğŸ¦ˆ', '~(_8^(I)', 'ğŸ˜µ', '(pi)', '>:â€‘)', '(T_T)', 'o->', '=\\', '(;o;)', 'o-o', '(*^0^*)', 'L-)', '=;', 'â˜¹ï¸', '(O)', ':â€‘p', ';]', '<\\3', 'ğŸŸ', ':!', '(x)', 'ğŸ™ˆ', ':#', 'ğŸ˜´', '8d', ':>', '(t_t)', 'ï¼ˆï¼¾ï¼ï¼¾ï¼‰', 'ğŸ‡®', ':-q', '[:|]', 'ğŸ–', '(^)', ':->', '(P)', '>:\\', '(a)', ';D', '7:^]', 'v.v.v', '\\U0001f590', ':~)', "Dâ€‘':", '^/^', '(st)', '(-:', '(s)', 'b-)', 'ğŸ˜¨', '|;-)', 'i-)', ':c)', ':O)', 'ğŸ¤•', '(Y)', 'x-D', 'ğŸ˜…', 'ğŸ˜¼', '%â€‘)', ':-3', ';-)', ':â€‘&', '\\U0001f620', '(T_T)/~~~', 'ğŸ˜Š', '(ãƒ»Ï‰ãƒ»)', ':b', 'ï¼ˆï¼¾â—‡ï¼¾ï¼‰', '\\U0001f518', ':-s', '(D)', '0:3', ':S', '(U)', '=_^=', ':l', '(o.o)', ":'â€‘(", 'I-)', '(^_-)', '*-:)', '\\U0001f4af', '\\o/', 'ğŸ˜¬', 'o=>', 'd:<', 'ãƒ½(^ã€‚^)ãƒ', '(#^.^#)', '($ãƒ»ãƒ»)/~~~', '((+_+))', ':-$', ':^o', '(M)', ':-?', 'o:)', 'ğŸ™„', '@-)', 'qq', '=d>', '(^^ã‚', '8â€‘0', '.....Ï†(ãƒ»âˆ€ãƒ»ï¼Š)', '=L', '>:{', ':[', ':/', '(W)', ':-,', '(^^)/~~~', 'ğŸ˜º', ';P', '~O)', 'D:', 'ğŸ™‚', '\\u0001f60a', ':"(', '(c)', ':-S', 'ğŸ˜Œ', '(f)', ':Ã—', '(A)', '(z)', '(^ã€‚^)', 'ğŸ³', '^Ï‰^', 'ğŸ˜©', ':ar!', ':Ã¾', 'ğŸ’¤', '(@_@)', '(^_^)/', '>_>^^<_<', '\\u0001f4af', 'ğŸ˜»', 'd8', '(X)', '<(__)>', '|â€‘O', 'ğŸ˜¾', ':@)', '(S)', '[..]', '>:(', ':-t', 'ğŸ˜·', '8->', '(^^)v', '_(_^_)_', '(l)', '**==', ':,(', ':â€‘Ã', '(F)', '0;^)', 'xd', '@>-->--', ':-h', '*\\0/*', ':â€‘###..', '^<_<', '(:|', '(t)', ':L', '=P', '>^_^<', 'ï¼ˆ*^_^*ï¼‰', '(*^â–½^*)', 'ğŸ™', 'ğŸ˜¶', ':â€‘<', '\\:d/', '\\U0001f1ee', '#:-s', '(*)', ':-)', '(-_-;)', ';-]', '<*)))â€‘{', '*<|:-)', '(k)', 'ï¼ˆï¿£â–¡ï¿£ï¼›ï¼‰', 'q_q', '5:â€‘)', '>:/', 'ğŸ’£', '\\u0001f1f3', '(^j^)', '(T)', '(n)', ':-X', 'v.v', '0:-3', '(ï½€Â´ï¼‰', 'ï¼ˆ*Â´â–½ï½€*ï¼‰', '(o|o)', '(ã‚œ_ã‚œ)', ':-O', 'ğŸ‘¾', 'ğŸ‹', 'ğŸ™ƒ', '<m(__)m>', '<:-p', ':â€‘x', '\\u0001f590', '(@^^)/~~~', '=(', '0:)', ':â€‘c', '~(_8^(i)', 'ï¼¼(~o~)ï¼', 'xD', 'ğŸ˜¢', '(^_^;)', '^:)^', ',:â€‘)', '^_^;', ':x', 'd;', ':$', ':â€‘(', ':s', ';â€‘)', 'X_X', '\\u0001f518', 'x(', '[-x', 'xâ€‘d', '^5', '(u)', 'ğŸ™', '(H)', '\\U0001f61c', 'ğŸ˜', ':\\*', '(h)', 'ğŸ˜', '8-x', 'D=', ':â€‘d', ':-l', '^^;', '|-o', 'B-)', ':Ã', '-d', '(G)', ':">', 'ğŸ˜¸', ';p', 'ğŸ»', ':-d', '|;â€‘)', '\\u0001f1ee', 'b^d', 'X-P', '<:o)', 'ğŸ˜°', '\\U0001f31f', '=l', 'oâ€‘o', 'ğŸ˜†', '*<|:â€‘)', '(ã‚œ-ã‚œ)', 'ğŸ¤', '\\u0001f31f', '*)', 'X-D', '0:-)', '>:)', 'dx', '\\u0001f604', 'ğŸ™', 'â˜•ï¸', ":'(", ':(:)', 'ğŸ˜‹', 'â˜¹', ':-<', '(â€˜A`)', '(;_;', "@}-;-'---", 'o-+', '>;)', ':â€‘,', ':-w', '}:)', 'ï¼¼(^o^)ï¼', ':-||', '=D', "dâ€‘':", 'xâ€‘D', '>:d<', ':â€‘b', 'ğŸ˜˜', 'ğŸ¤¢', ';)', '(m)', '>:o', '(L)', 'ğŸ™‡', '(pl)', 'ğŸ¤’', '3:-)', '(ï¼›ä¸€_ä¸€)', 'QQ', '\\m/', 'ğŸ˜­', '#â€‘)', '\\u0001f44b', '(e)', '=d', ':-b', ":')", '(ap)', '<:-P', 'ğŸ‘¼', ':(|)', '(;_;)', '=-d', 'ğŸš¬', '~:-\\', '>:O', 'ğŸŒŸ', ':-/', 'Â§^ã€‚^Â§', '^m^', 'T.T', 'O-O', '(*^3^)/~â˜†', '3:-O', 'ğŸ˜›', ':c', '=)', '\\U0001f60a', ':-c', 'ğŸ˜š', 'ğŸ™‹', '(@)', '(~)', '8D', '(ï½”â–½ï½”)', ';_;', 'ï¼ˆï¿£ãƒ¼ï¿£ï¼‰', '\\U0001f3fb', 'ğŸ˜‘', 'D;', '(ãƒ»ãƒ»;)', ':-bd', '>-)', 'ğŸ˜‚', '~@~', 'Xâ€‘D', 'ğŸ˜', '/:)', '>:', '>:P', '(^_^)', '(ãƒ»ã¸ãƒ»)', '<*)))-{', '(^O^)ï¼', 'ğŸ˜•', '[-X', 'ğŸ¬', ':-SS', 'ï¼ˆâ—ï¼¾oï¼¾â—ï¼‰', 'ï¼ˆï¼¾ï½•ï¼¾ï¼‰', 'o:â€‘)', '(@_@ã€‚', '(ãƒ¼ãƒ¼;)', 'ğŸ’ƒ', 'X(', ':-o', '(ã‚œoã‚œ)', '(*ï¿£mï¿£)', ':-D', '(*_*;', ':D', 'ğŸ˜', '(^^)/', ":'â€‘)", 'O:-)', 'ï¼ˆï¼¾ï½–ï¼¾ï¼‰', 'ğŸ¡', '(â‰§âˆ‡â‰¦)/', 'ğŸ™', 'ğŸ˜§', ':3', '(p_-)', ':-B', '(-;', '(..)', ':-}', ';o)', '(+o+)', '(~~)', ':-|', '(V)oï¿¥o(V)', '<:â€‘|', 'Q.Q', ';;)', '(^o^)', '5:-)', '8-}', '(mo)', '(i)', '(/_;)', 'x-(', 'â˜»', ':â€‘D', 'O_O', '[-O<', '(ï¼ï¼ã€†)', '%-)', '!(^^)!', '(ã‚œã‚œ)', '(tot)/~~~', '(=^ãƒ»^=)', ':-J', 'D:<', 'âœŒï¸', '(^O^)', '8-|', '(~_~)']

# Takes tokenised tweet as input argument and return the list of emoticons
# present in the tweet.
def GetEmoticons(tweet):
	emoticons=[]
	for emoticon in all_emoticons:
		if emoticon in tweet:
			emoticons.append(emoticon)
	return emoticons

def give_emoji_free_text(text):
	return emoji.get_emoji_regexp().sub(u'', text)

def remove_emoticon(tweet):
	tweet = p.clean(tweet)
	'''delims = GetEmoticons(BaseTokeniser(tweet))
	for delim in delims:
		tweet = tweet.replace(delim,"")'''
	tweet = give_emoji_free_text(tweet)
	return tweet

def remove_tags(doc):
	doc = ' '.join(word for word in doc.split() if word[0]!='<')
	return doc

def remove_slang(sent):
	return [slangdict[w.strip()] if w.strip() in slangdict else w for w in sent]

# Takes tokenised tweet as input argument and return the list of hash tags
# present in the tweet.
def GetHashTags(tweet):
	hashtags = []
	for token in tweet:
		if token[0]=='#':
			hashtags.append(token)
	return hashtags

# Takes tokenised tweet as input argument and return the list of usernames
# present in the tweet.
def GetUserNames(tweet):
	user_names = []
	for token in tweet:
		if token[0]=='@':
			user_names.append(token)
	return user_names

# Takes tweet as input argument and return the list of URL's present
# in the tweet.
def GetURLs(tweet,lowercase=False):
	url_regex = [r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-f][0-9a-f]))+']
	url_re = re.compile(r'('+'|'.join(url_regex)+')', re.VERBOSE | re.IGNORECASE)
	urls = url_re.findall(tweet)
	if lowercase:
		for x in range(len(urls)):
			urls[x] = urls[x].lower()
	return urls

# Remove punctuation marks from a preprocessed tweet.
def RemovePunctuations(processed_tweet):
	punctuations_marks = string.punctuation
	punctuations_marks = punctuations_marks+'Â¿'+"à¥¤"+"|"+'â€”'+'ãƒ»'+'â€¦'
	processed_tweet = processed_tweet.translate(str.maketrans('','',punctuations_marks))
	return processed_tweet

# Split the tweet into tokens.
def BaseTokeniser(tweet,lowercase=False):
	if lowercase:
		tweet = tweet.lower()
	tokenised_tweet = tweet.split(' ')
	tokenised_tweet = list(filter(None, tokenised_tweet))
	return tokenised_tweet


def ReplaceTwoOrMore(word):
	return re.sub(r'(.)\1+', r'\1\1', word)

def RemoveRepetitions(tweet):
	processed_tweet = tweet
	for i in range(len(processed_tweet)):
		word = processed_tweet[i]
		if processed_tweet[i] != ReplaceTwoOrMore(word):
			processed_tweet[i] = ReplaceTwoOrMore(word)
	return processed_tweet


def removeRepeat(string):
	return re.sub(r'(.)\1+', r'\1\1', string)

def RemoveNumbers(tweet):
	return re.sub(r'\d+', '',tweet)

def CustomPreprocessor(tweet,lowercase=False):
	tokenised_tweet = BaseTokeniser(tweet,lowercase)
	emoticons = GetEmoticons(tokenised_tweet)
	hashtags = GetHashTags(tokenised_tweet)
	usernames = GetUserNames(tokenised_tweet)
	urls = GetURLs(tweet,lowercase)
	delims=emoticons+hashtags+usernames+urls
	processed_tweet = ' '.join(tokenised_tweet)
	for delim in delims:
		processed_tweet = processed_tweet.replace(delim,"")
	processed_tweet = RemovePunctuations(processed_tweet)
	processed_tweet = RemoveNumbers(processed_tweet)
	processed_tweet_tokenised = BaseTokeniser(processed_tweet)
	processed_tweet= RemoveRepetitions(processed_tweet_tokenised)
	return ' '.join(processed_tweet).strip()


def save_model(file_path,model):
	file = open(file_path,"wb")
	pickle.dump(model,file)
	file.close()

def load_model(file_path):
	file = open(file_path,"rb")
	model = pickle.load(file)
	file.close()
	return model

def write_in_file(filename,data):
	print("Writing content in file....")
	fd = open(filename,"w",encoding="utf-8")
	fd.write("\n".join(data))
	fd.close()
	print("Done!!")


def is_english(s):
	try:
		s.encode(encoding='utf-8').decode('ascii')
	except UnicodeDecodeError:
		return False
	else:
		return True

def check_en_lang(text):
	doc = nlp(text)
	flag=True
	if doc._.language["language"]!="en":
		flag=False
	else:
		pass
		#for token in doc:
			#if token._.language["language"]!="en":
				#flag=False
				#break
	return flag

def get_sentences(data):
	##data :- list of text (paragraph)
	temp=[]
	for text in data:
		text_sentences = nlp1(text)
		for sentence in text_sentences.sents:
			temp.append(sentence.text)
	return temp

def TextPreprocessing(data,text_processor,sentence_segmentation=False,pre_lang_check=True,mode=0):
	global MIN_TOKENS
	sents = [fix_text(sent) for sent in data]
	if pre_lang_check:
		sents = [sent for sent in sents if is_english(sent)]
	if sentence_segmentation:
		sents = get_sentences(sents)
	sents = [remove_emoticon(" ".join(remove_slang(sent.split()))) for sent in sents]
	sents = [TwitterPreprocessor(" ".join(sent)).remove_blank_spaces().text for sent in list(text_processor.pre_process_docs(sents))]
	sents = [sent.replace("<censored>","").replace("<emphasis>","").replace("<elongated>","").replace("<repeated>","") for sent in sents]
	if mode:
		MIN_TOKENS*=2
	sents = []
	for sent in sents:
		str_=''
		sent = RemovePunctuations(sent)
		for word in sent.split():
			word = word.strip()
			if word:
				if len(list(word))==1:
					if word in ('a','e','i','o','u'):
						str_+=word+" "
				else:
					str_+=word+" "
		sent=str_.strip()
		if len(sent.split())>=MIN_TOKENS and len(sent.split())<=MAX_TOKENS:
			sents.append(sent)
	sents = [sent for sent in sents if check_en_lang(sent)]
	return sents

def get_mode(data_type):
	mode=0
	if data_type=="post":
		mode=1
	return mode

def get_preprocessed_data(raw_data,sentence_segmentation=False,pre_lang_check=True):
	text_processor = TextPreProcessor(
	omit=['url', 'email', 'user'],
	normalize=['url', 'email', 'user'],
	annotate={"elongated", "repeated",'emphasis', 'censored'},
	segmenter="twitter",
	corrector="twitter",
	unpack_hashtags=True,
	unpack_contractions=True,
	spell_correct_elong=False,
	spell_correction=False,
	tokenizer=SocialTokenizer(lowercase=True).tokenize)
	processed_data=[]
	for data_type in raw_data:
		if len(raw_data[data_type])>=MAX_SEQ:
			chunks = int(len(raw_data[data_type])/MAX_SEQ)
			for j in range(chunks):
				processed_data+=TextPreprocessing(raw_data[data_type][MAX_SEQ*j:MAX_SEQ*(j+1)],text_processor,sentence_segmentation,pre_lang_check,get_mode(data_type))
			if MAX_SEQ*chunks!=len(raw_data[data_type]):
				processed_data+=TextPreprocessing(raw_data[data_type][MAX_SEQ*chunks:],text_processor,sentence_segmentation,pre_lang_check,get_mode(data_type))
		else:
			processed_data+=TextPreprocessing(raw_data[data_type],text_processor,sentence_segmentation,pre_lang_check,get_mode(data_type))
	return processed_data

DATA_TYPE="tw"
if DATA_TYPE in ("tw","insta","fb"):
	DATA_PATH = "../data/raw_data/"+DATA_TYPE

data=defaultdict(list)
comments=False
for file in os.listdir(DATA_PATH):
	FILE_PATH=os.path.join(DATA_PATH,file)
	if DATA_TYPE=="tw":
		if file=="tweets.txt":
			for sent in open(FILE_PATH,"r").read().split("\n"):
				sent = sent.strip()
				if sent:
					temp  = ",".join(sent.split(",")[1:])
					if temp not in data['comment']:
						data['comment'].append(temp)
	elif DATA_TYPE=="fb":
		if file=="fb_posts" or file=="fb_comments":
			for sent in open(FILE_PATH,"r").read().split("--------------------"):
				sent = sent.strip()
				if sent:
					temp = ",".join(sent.strip().split(",")[1:]).replace("\n","")
					if 'post' in file:
						if temp not in data['post']:
							data['post'].append(temp)
					else:
						if temp not in data['comment']:
							data['comment'].append(temp)
	else:
		if file=="insta_captions" or file=="insta_comments":
			for sent in open(FILE_PATH,"r").read().split("--------------------"):
				sent=sent.strip()
				if sent:
					temp = ":".join(sent.strip().split(":")[1:]).replace("\n","")
					if 'caption' in file:
						if temp not in data['post']:
							data['post'].append(temp)
					else:
						if temp not in data['comment']:
							data['comment'].append(temp)

print("Total posts/captions in raw_data: ",len(data['post']))
print("Total comments in raw_data: ",len(data['comment']))
print("Total sequences in raw_data: ", len(data['post'])+len(data['comment']))
DATA_PATH = "../data/preprocessed_data/"+DATA_TYPE
if not os.path.exists(DATA_PATH):
	os.makedirs(DATA_PATH)

data = get_preprocessed_data(data)
preprocessed_filename=DATA_TYPE+"_data"
write_in_file(os.path.join(DATA_PATH,preprocessed_filename),data)
