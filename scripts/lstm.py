# -*- coding: utf-8 -*-
"""instagram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-_7eaYStgedaUG03-Gu3vqEalE0Cgysb
"""

from keras.preprocessing.sequence import pad_sequences
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
from keras.callbacks import EarlyStopping
from keras.models import Sequential
import keras.utils as ku
import numpy as np
import pickle
from keras import optimizers
#from keras.utils import multi_gpu_model
import time
import os
#os.environ['THEANO_FLAGS']='device=gpu'
from keras.models import load_model

tokenizer = Tokenizer()

def dataset_preparation(data):
    corpus = data
    # corpus = []
    # for line in data:
    #     l = len(line.split(" "))
    #     if(l>150):
    #         line_temp = line.split(" ")
    #         n = l/150
    #         c = 0
    #         while(c!=n):
    #             if(c==n-1):
    #                 corpus.append(" ".join(line_temp[(c*150):]))
    #             else:
    #                 corpus.append(" ".join(line_temp[(c*150):(c*150)+150]))
    #             c += 1
    #     else:
    #         corpus.append(line)


#     corpus = data.lower().split("\n")
#     corpus = data.lower()
    tokenizer.fit_on_texts(corpus)
    total_words = len(tokenizer.word_index) + 1
    input_sequences = []
    count = 0
    for line in corpus:
        if(count<5):
            print(line)
        count += 1
        if((count%500)==0):
            print(count)
        token_list = tokenizer.texts_to_sequences([line])[0]
        for i in range(1,len(token_list)):
            n_gram_sequence = token_list[:i+1]
            input_sequences.append(n_gram_sequence)
    max_sequence_len = max([len(x) for x in input_sequences])
    print("done1")
    input_sequences = pad_sequences(input_sequences,maxlen = max_sequence_len, padding = 'pre')
    print("done1/2")
    #time.sleep(10)
    input_sequences = np.array(input_sequences)
    print("done2")
    # print("number of input sequences" , len(input_sequences))
    predictors = input_sequences[:,:-1]
    print("done3")
    labels = input_sequences[:,-1]
    print("done4")
    # labels = ku.to_categorical(labels,num_classes=total_words)
    print("done5")
    # saving
    with open("../lm_models/dl_models/tw_tokenizer.pkl",'wb') as handle:
        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
    return predictors,labels,max_sequence_len,total_words

def create_model(predictors,labels,max_sequence_len,total_words):
    input_len = max_sequence_len - 1
    print("input length" , input_len)
    model = Sequential()
    model.add(Embedding(total_words, 100 , input_length = input_len))
#     model.add(LSTM(512,input_shape = (None,input_len,100),return_sequences=True))
#     model.add(LSTM(512,input_shape = (None, 512)))
    # model.add(LSTM(150,return_sequences=True))
    model.add(LSTM(50))
    model.add(Dropout(0.1))
    model.add(Dense(total_words, activation='softmax'))
    # model.compile(loss='categorical_crossentropy',optimizer='adam')
    opt_adam = optimizers.adam(lr=0.001)
    #model = multi_gpu_model(model,gpus=2)
    model.compile(loss='sparse_categorical_crossentropy',
        metrics=['sparse_categorical_accuracy'],optimizer=opt_adam)
    model.fit(predictors,labels,batch_size=128,epochs=100,verbose=1)
    print(model.summary())
    model.save('../lm_models/dl_models/tw_model.h5')
    return model

def generate_model(input_words,no_of_next_words,max_sequence_len,model):
    for j in range(no_of_next_words):
        token_list = tokenizer.texts_to_sequences([input_words])[0]
        token_list = pad_sequences([token_list],maxlen = max_sequence_len-1,padding='pre')
        predicted = model.predict_classes(token_list,verbose = 0)
        output_word = ""
        for word,index in tokenizer.word_index.items():
            if(index == predicted):
                output_word = word
                break
        input_words += " "+output_word
    return input_words
with open('../model_data/tw/dev_sents.pkl', 'rb') as handle:
    import dill as pickle
    data = pickle.load(handle)

predictors,labels,max_sequence_len,total_words = dataset_preparation(data[:10000])
print("done")
data.clear()

model = create_model(predictors,labels,max_sequence_len,total_words)




