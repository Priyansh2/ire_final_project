from ekphrasis.classes.preprocessor import TextPreProcessor
from ekphrasis.classes.tokenizer import SocialTokenizer
from ekphrasis.utils.nlp import polarity
from ekphrasis.dicts.emoticons import emoticons
from ekphrasis.dicts.noslang.slangdict import slangdict
from ftfy import fix_text
import preprocessor as p
import emoji
import demoji
demoji.download_codes()
from twitter_preprocessor import TwitterPreprocessor
p.set_options(p.OPT.EMOJI,p.OPT.RESERVED,p.OPT.SMILEY)
all_emoticons = ['(ï¼ _ï¼ ;)', '~:>', '=p', ':â€‘.', '|-)', ':-]', '=P~', 'b-(', 'ğŸ˜œ', '(^â—‹^)', '(v)oï¿¥o(v)', 'o:-)', 'ğŸ™†', '(^_-)-â˜†', ':-\\', '(ï¼´â–½ï¼´)', '(*_*)', '(w)', ':o(', '<`ãƒ˜Â´>', 'à² _à² ', ';(', '(^ãƒ»^)', '(âœ¿â— â€¿â— )', ':)', '3:)', 'ğŸ˜—', 'ğŸ˜', '(/â—•ãƒ®â—•)/', 'ğŸ˜”', ':^)', 'x-p', '=-3', 'ğŸ™Š', '_(._.)_', '(^.^)/~~~', ":'-)", '8-0', ':((', 'ğŸ‚', '=D>', '>:p', ':-"', '(?_?)', 'ğŸ¤“', '(so)', 'o_O', ':)>-', ':â€‘j', 'ğŸ˜½', 't.t', '\\U0001f604', '^_^', ';â€‘]', 'ğŸ‘¶', 'ğŸ™', ',:-)', ':-L', ':-p', ':!!', '(^o^)ï¼', '[-(', 'ğŸ˜±', '(ï¼â€¸áƒš)', 'd=', '(+_+)', 'x-d', '=p~', '#:-S', ':â€‘|', ':â€‘Ã¾', 'ğŸ˜‡', ';^)', '=-D', 'ğŸ˜£', ':o3', '(~o~)', '(C)', 'ğŸ˜¿', "+/'\\", ':-j', 'ï¼ˆÂ´âˆ€ï½€ï¼‰', '%-(', '8-d', '0:â€‘3', '(Â´ï½¥Ï‰ï½¥`)', '(-""-)', '$-)', 'ï¼¼(â—oâ—)ï¼ï¼', 'ğŸ˜¥', ';d', ':-Ã¾', '<3', '<^!^>', '(;', '(Z)', 'ğŸ™…', ':###..', 'Q_Q', 'ğŸ˜’', '(._.)', '=:)', 'XP', '(6)', '=:o]', '(ï¼¾â–½ï¼¾)', '=((', '(:_;)', ':â€‘)', '<(ï½€^Â´)>', ':o)', '(I)', 'ğŸ‘½', 'ğŸ§', 'x-@', '(B)', '(^<^)', '(^^)', ':â€‘/', '@>â€‘â€‘>â€‘â€‘', "D-':", 'o/\\o', '=))', 'ğŸ˜™', '(E)', '\\U0001f44b', 'ğŸµ', 'â˜„ï¸', ':\\', '>:s', 'ğŸ˜„', ':-ss', '(>_<)>', ':-??', ';))', '(y)', ';-;', 'O:â€‘)', 'ğŸ˜‰', '(N)', 'ï¼ˆâ€™-â€™*)', 'ğŸ˜«', ':]', '@}->--', ':ã£c', '<):)', 'ğŸ˜®', '^o)', '(*^^)v', ':-#', ':^*', '>.<', '}:â€‘)', '(p)', 'Xâ€‘P', ':-(', '(ã‚œ_ã‚œ>)', ':-[', '8)', '(d)', '(o)', ':â€‘O', '(ãƒ»ãƒ»?', 'Â°OÂ°', ':p', 'ğŸ˜Ÿ', 'ğŸ˜¹', '(^)o(^)', '%%-', 'V.v.V', 'xâ€‘p', '(-_-ãƒ¡)', "d-':", '(ä¸€ä¸€)', '(li)', ':-P', ':o', '(__)', '<`ï½Â´>', '|-O', '(*^ã€‚^*)', 'ãƒ½(^o^)ä¸¿', 'ğŸ˜¤', '\\u0001f3fb', '(ToT)', 'o_o', ':X', ':|', '(8)', '(^ãƒ ^)', ':-*', '=/', 'DX', '(-_-)zzz', '~X(', '\\U0001f1f3', 'XD', '8-X', '#-o', "(';')", '(=^ãƒ»ãƒ»^=)', '(ãƒ¼ãƒ¼ã‚›)', ';n;', ':}', 'ğŸ˜€', 'O_o', '(&)', ':P', 'ğŸ˜“', 'ğŸ™Œ', ':â€‘J', ':ã£)', 'ğŸ™‰', '3:-o', '[-o<', '(^_^ãƒ¡)', ':â€‘X', 'xp', '\\U0001f64f', 'ğŸ¤”', '(sn)', '\\u0001f620', 'ğŸ™€', '8â€‘d', 'd:', '(^_^)/~', '+o(', '|â€‘o', ':-))', 'âˆ©(ãƒ»Ï‰ãƒ»)âˆ©', ':-&', ':â€‘o', '(||)', ':{', '(*Â°âˆ€Â°)=3', '>_>^', '\\:D/', '(~_~;)', '\\u0001f602', '8o|', '(})', ':<', 'ğŸ', '(ãƒ¼_ãƒ¼)!!', '>:-)', '\\u0001f64f', 'ğŸ’¯', 'ğŸ‘‹', 'l-)', '(~_~ãƒ¡)', '\\u0001f3c2', '(um)', "@}â€‘;â€‘'â€‘â€‘â€‘", '(:', 'Â°oÂ°', 'm(__)m', 'â˜ºï¸', '(-.-)', 'ğŸ˜³', ':â€‘#', '(-_-)', ':))', '8â€‘D', '~:â€‘\\', 'ğŸ˜–', '><>', '3:â€‘)', '>:D<', '(^J^)', 'ğŸ”˜', '</3', ':-x', '(^o^)ä¸¿', 'ï¼¼(-o-)ï¼', '(ã‚œ.ã‚œ)', '(^0_0^)', '><(((*>', ':â€‘[', 'D8', '>:S', '~o)', '(co)', 'ğŸ˜¡', '//0-0\\\\', "('_')", '(mp)', '<:-|', '(=_=)', 'x_x', '}:-)', '(g)', '//0â€‘0\\\\', ';-D', '(^_^.)', '=3', ':-.', '(^^;)', '(â€˜a`)', ':-@', 'ğŸ˜²', 'q.q', ')^o^(', 'ğŸ˜¦', '\\U0001f3c2', '8-)', 'ğŸ ', '(;_;)/~~~', 'ğŸ˜¯', 'ğŸ¦‡', '(;_:)', '8-D', '^#(^', '(t_t)/~~~', '(;O;)', '-D', ':O', 'ğŸ˜', ':@', ':-0', '<(o0o)>', '((d[-_-]b))', '(%)', '*-)', '>:[', '(^.^)', ':-###..', 'ğŸ˜ª', '0:â€‘)', ':&', 'â˜º', '(ãƒ»_ãƒ»;)', '(tot)', '(ip)', ':(', 'ğŸ˜ƒ', '(au)', 'o_0', '\\u0001f61c', '\\U0001f602', 'B^D', ':-Ã', '@};-', ':^(', 'ğŸ˜ ', '(^_^)v', ':d', 'ï¼ˆâŒ’â–½âŒ’ï¼‰', '(ã‚œãƒ¬ã‚œ)', ':â€‘P', '(ToT)/~~~', ':*', 'ğŸ˜', ':bz', '(bah)', 'o.O', ';-d', '%)', ":'-(", ':ã£C', 'ğŸ˜ˆ', 'o.o', '(>_<)', '(K)', 'ğŸ‡³', 'ğŸ¤‘', 'ğŸ±', '~x(', '(b)', 'ğŸ¤¦', '(ãƒ»ã€‚ãƒ»;)', '=]', '#-)', 'O:)', '({)', ':)]', 'ï¼ˆãƒ»âˆ€ãƒ»ï¼‰', 'ğŸ¦ˆ', '~(_8^(I)', 'ğŸ˜µ', '(pi)', '>:â€‘)', '(T_T)', 'o->', '=\\', '(;o;)', 'o-o', '(*^0^*)', 'L-)', '=;', 'â˜¹ï¸', '(O)', ':â€‘p', ';]', '<\\3', 'ğŸŸ', ':!', '(x)', 'ğŸ™ˆ', ':#', 'ğŸ˜´', '8d', ':>', '(t_t)', 'ï¼ˆï¼¾ï¼ï¼¾ï¼‰', 'ğŸ‡®', ':-q', '[:|]', 'ğŸ–', '(^)', ':->', '(P)', '>:\\', '(a)', ';D', '7:^]', 'v.v.v', '\\U0001f590', ':~)', "Dâ€‘':", '^/^', '(st)', '(-:', '(s)', 'b-)', 'ğŸ˜¨', '|;-)', 'i-)', ':c)', ':O)', 'ğŸ¤•', '(Y)', 'x-D', 'ğŸ˜…', 'ğŸ˜¼', '%â€‘)', ':-3', ';-)', ':â€‘&', '\\U0001f620', '(T_T)/~~~', 'ğŸ˜Š', '(ãƒ»Ï‰ãƒ»)', ':b', 'ï¼ˆï¼¾â—‡ï¼¾ï¼‰', '\\U0001f518', ':-s', '(D)', '0:3', ':S', '(U)', '=_^=', ':l', '(o.o)', ":'â€‘(", 'I-)', '(^_-)', '*-:)', '\\U0001f4af', '\\o/', 'ğŸ˜¬', 'o=>', 'd:<', 'ãƒ½(^ã€‚^)ãƒ', '(#^.^#)', '($ãƒ»ãƒ»)/~~~', '((+_+))', ':-$', ':^o', '(M)', ':-?', 'o:)', 'ğŸ™„', '@-)', 'qq', '=d>', '(^^ã‚', '8â€‘0', '.....Ï†(ãƒ»âˆ€ãƒ»ï¼Š)', '=L', '>:{', ':[', ':/', '(W)', ':-,', '(^^)/~~~', 'ğŸ˜º', ';P', '~O)', 'D:', 'ğŸ™‚', '\\u0001f60a', ':"(', '(c)', ':-S', 'ğŸ˜Œ', '(f)', ':Ã—', '(A)', '(z)', '(^ã€‚^)', 'ğŸ³', '^Ï‰^', 'ğŸ˜©', ':ar!', ':Ã¾', 'ğŸ’¤', '(@_@)', '(^_^)/', '>_>^^<_<', '\\u0001f4af', 'ğŸ˜»', 'd8', '(X)', '<(__)>', '|â€‘O', 'ğŸ˜¾', ':@)', '(S)', '[..]', '>:(', ':-t', 'ğŸ˜·', '8->', '(^^)v', '_(_^_)_', '(l)', '**==', ':,(', ':â€‘Ã', '(F)', '0;^)', 'xd', '@>-->--', ':-h', '*\\0/*', ':â€‘###..', '^<_<', '(:|', '(t)', ':L', '=P', '>^_^<', 'ï¼ˆ*^_^*ï¼‰', '(*^â–½^*)', 'ğŸ™', 'ğŸ˜¶', ':â€‘<', '\\:d/', '\\U0001f1ee', '#:-s', '(*)', ':-)', '(-_-;)', ';-]', '<*)))â€‘{', '*<|:-)', '(k)', 'ï¼ˆï¿£â–¡ï¿£ï¼›ï¼‰', 'q_q', '5:â€‘)', '>:/', 'ğŸ’£', '\\u0001f1f3', '(^j^)', '(T)', '(n)', ':-X', 'v.v', '0:-3', '(ï½€Â´ï¼‰', 'ï¼ˆ*Â´â–½ï½€*ï¼‰', '(o|o)', '(ã‚œ_ã‚œ)', ':-O', 'ğŸ‘¾', 'ğŸ‹', 'ğŸ™ƒ', '<m(__)m>', '<:-p', ':â€‘x', '\\u0001f590', '(@^^)/~~~', '=(', '0:)', ':â€‘c', '~(_8^(i)', 'ï¼¼(~o~)ï¼', 'xD', 'ğŸ˜¢', '(^_^;)', '^:)^', ',:â€‘)', '^_^;', ':x', 'd;', ':$', ':â€‘(', ':s', ';â€‘)', 'X_X', '\\u0001f518', 'x(', '[-x', 'xâ€‘d', '^5', '(u)', 'ğŸ™', '(H)', '\\U0001f61c', 'ğŸ˜', ':\\*', '(h)', 'ğŸ˜', '8-x', 'D=', ':â€‘d', ':-l', '^^;', '|-o', 'B-)', ':Ã', '-d', '(G)', ':">', 'ğŸ˜¸', ';p', 'ğŸ»', ':-d', '|;â€‘)', '\\u0001f1ee', 'b^d', 'X-P', '<:o)', 'ğŸ˜°', '\\U0001f31f', '=l', 'oâ€‘o', 'ğŸ˜†', '*<|:â€‘)', '(ã‚œ-ã‚œ)', 'ğŸ¤', '\\u0001f31f', '*)', 'X-D', '0:-)', '>:)', 'dx', '\\u0001f604', 'ğŸ™', 'â˜•ï¸', ":'(", ':(:)', 'ğŸ˜‹', 'â˜¹', ':-<', '(â€˜A`)', '(;_;', "@}-;-'---", 'o-+', '>;)', ':â€‘,', ':-w', '}:)', 'ï¼¼(^o^)ï¼', ':-||', '=D', "dâ€‘':", 'xâ€‘D', '>:d<', ':â€‘b', 'ğŸ˜˜', 'ğŸ¤¢', ';)', '(m)', '>:o', '(L)', 'ğŸ™‡', '(pl)', 'ğŸ¤’', '3:-)', '(ï¼›ä¸€_ä¸€)', 'QQ', '\\m/', 'ğŸ˜­', '#â€‘)', '\\u0001f44b', '(e)', '=d', ':-b', ":')", '(ap)', '<:-P', 'ğŸ‘¼', ':(|)', '(;_;)', '=-d', 'ğŸš¬', '~:-\\', '>:O', 'ğŸŒŸ', ':-/', 'Â§^ã€‚^Â§', '^m^', 'T.T', 'O-O', '(*^3^)/~â˜†', '3:-O', 'ğŸ˜›', ':c', '=)', '\\U0001f60a', ':-c', 'ğŸ˜š', 'ğŸ™‹', '(@)', '(~)', '8D', '(ï½”â–½ï½”)', ';_;', 'ï¼ˆï¿£ãƒ¼ï¿£ï¼‰', '\\U0001f3fb', 'ğŸ˜‘', 'D;', '(ãƒ»ãƒ»;)', ':-bd', '>-)', 'ğŸ˜‚', '~@~', 'Xâ€‘D', 'ğŸ˜', '/:)', '>:', '>:P', '(^_^)', '(ãƒ»ã¸ãƒ»)', '<*)))-{', '(^O^)ï¼', 'ğŸ˜•', '[-X', 'ğŸ¬', ':-SS', 'ï¼ˆâ—ï¼¾oï¼¾â—ï¼‰', 'ï¼ˆï¼¾ï½•ï¼¾ï¼‰', 'o:â€‘)', '(@_@ã€‚', '(ãƒ¼ãƒ¼;)', 'ğŸ’ƒ', 'X(', ':-o', '(ã‚œoã‚œ)', '(*ï¿£mï¿£)', ':-D', '(*_*;', ':D', 'ğŸ˜', '(^^)/', ":'â€‘)", 'O:-)', 'ï¼ˆï¼¾ï½–ï¼¾ï¼‰', 'ğŸ¡', '(â‰§âˆ‡â‰¦)/', 'ğŸ™', 'ğŸ˜§', ':3', '(p_-)', ':-B', '(-;', '(..)', ':-}', ';o)', '(+o+)', '(~~)', ':-|', '(V)oï¿¥o(V)', '<:â€‘|', 'Q.Q', ';;)', '(^o^)', '5:-)', '8-}', '(mo)', '(i)', '(/_;)', 'x-(', 'â˜»', ':â€‘D', 'O_O', '[-O<', '(ï¼ï¼ã€†)', '%-)', '!(^^)!', '(ã‚œã‚œ)', '(tot)/~~~', '(=^ãƒ»^=)', ':-J', 'D:<', 'âœŒï¸', '(^O^)', '8-|', '(~_~)']
def GetEmoticons(tweet):
	emoticons=[]
	for emoticon in all_emoticons:
		if emoticon in tweet:
			emoticons.append(emoticon)
	return emoticons
def BaseTokeniser(tweet,lowercase=False):
	if lowercase:
		tweet = tweet.lower()
	tokenised_tweet = tweet.split(' ')
	tokenised_tweet = list(filter(None, tokenised_tweet))
	return tokenised_tweet

def give_emoji_free_text(text):
	return emoji.get_emoji_regexp().sub(u'', text)

def remove_emoticon(tweet):
	tweet = p.clean(tweet)
	'''delims = GetEmoticons(BaseTokeniser(tweet))
	for delim in delims:
		tweet = tweet.replace(delim,"")'''
	tweet = give_emoji_free_text(tweet)
	return tweet

sentences = [
	"_waga i can honestly say i wasn't playing this whole time. promise!",
	"So there is no way for me to plug it in here in the US unless I go by a converter.",
	"Thanks x https://t.co/ZXTcDLyDS9",
	"I saw the new #JOHNDOE movie AND IT SUCKS!!! WAISTED $10... #badmovies :/",
	"Good case, Excellent value. 100%",
	"Works great!",
	'The design is very odd, as the ear "clip" is not very comfortable at all.',
	"Needless to say, I wasted my money.",
	 "CANT WAIT for the new season of #TwinPeaks ï¼¼(^o^)ï¼ yaaaay!!! #davidlynch #tvseries :)))",
	"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies >3:/",
	"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! >:-D http://sentimentsymposium.com/.",
	"@Calum5SOS You lil *poop* please follow @EmilyBain224 â˜ºï¸ğŸ’•",
	"Words attendees would use to describe @prosper4africa's #ALN 2015! https://t.co/hmNm8AdwOh",
	 "@TheTideDrew Hi, Drew! I can't wait to see you!â˜º Just letting you know that you'll always be my spidey, I love you!ğŸ’• Mind following me? x215",
	 "Welcome to http://t.co/3YNnqG23S9(SAMSUNG,Kodak,bp,Coca-cola,Holcim,Nestle) 26.11.2013( http://t.co/9Xp7K7g3UW MAILAN(KISSME) SINCE 1969",
	 "canbe foundathttp://www.osp.gatech.edu/rates/(http://www.osp.gatech.edu/rates/).",
	 "[https://link.springer.com/article/10.1007/s10940\\-016\\-9314\\-9](https://link.springer.com/article/10.1007/s10940-016-9314-9)",
	 "Call me on +917893774735 and 7893774735 is my number 888-222-111 10PM 2 pm 3 P.M f**k",
	 "<html>hello</html>, <body></body> , </br>",
	 "2getr"
]

text_processor = TextPreProcessor(
	# terms that will be normalized
	#omit=["percent","money","phone","time","date","number"],
	omit=['url', 'email', 'user'],
	normalize=['url', 'email', 'user'],

	#normalize=['url', 'email', 'user',"percent","money","phone","time","date","number"],
	# terms that will be annotated
	annotate={"elongated", "repeated",'emphasis', 'censored'},
	fix_bad_unicode = True,
	fix_text=True,
	fix_html=True,
	# corpus from which the word statistics are going to be used
	# for word segmentation
	segmenter="twitter",

	# corpus from which the word statistics are going to be used
	# for spell correction
	corrector="twitter",

	unpack_hashtags=True,  # perform word segmentation on hashtags
	unpack_contractions=True,  # Unpack contractions (can't -> can not)
	spell_correct_elong=False,  # spell correction for elongated words
	spell_correction=False,
	# select a tokenizer. You can use SocialTokenizer, or pass your own
	# the tokenizer, should take as input a string and return a list of tokens
	tokenizer=SocialTokenizer(lowercase=True).tokenize,

	# list of dictionaries, for replacing tokens extracted from the text,
	# with other expressions. You can pass more than one dictionaries.
)
def remove_tags(doc):
	"""
	Remove tags from sentence
	"""
	doc = ' '.join(word for word in doc.split() if word[0]!='<')
	return doc

def remove_slang(sent):
	return [slangdict[w.strip()] if w.strip() in slangdict else w for w in sent]

sents = [remove_emoticon(" ".join(remove_slang(sent.split()))) for sent in sentences]
tokenized_sentences = list(text_processor.pre_process_docs(sents))
for x in range(len(tokenized_sentences)):
	sent = " ".join(tokenized_sentences[x])
	sent = TwitterPreprocessor(sent).remove_blank_spaces().text
	print(sentences[x])
	print(sent,"\n\n")
